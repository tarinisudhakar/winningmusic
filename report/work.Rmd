---
title: "What goes into a record-breaking song?"
subtitle: "ECO 395 Final Project"
author: "Soo Jee Choi, Annie Nguyen, Tarini Sudhakar"
date: "2023-04-10"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
	message = FALSE,
	warning = FALSE,
	cache = TRUE
)
library(tidyverse)
library(dplyr)
library(rsample)
library(modelr)
library(pROC)
library(foreach)
library(mosaic)
library(lubridate)
library(tibble)
library(ggcorrplot)
library(parallel)
library(caret)
library(randomForest)
library(rpart)
library(rpart.plot)
library(doParallel)
library(ClusterR) # new for kmeans++
library(fastDummies)
library(tibble)
library(tidyr)
```

##  I. Abstract

How do you make a hit song? Intuitively, a hit song should have a good beat, a catchy hook, and not-too-complicated lyrics. But we can do better than that, given the amount of data surrounding us. Spotify has been able to break a song into quantitative features such as energy, tempo, acoustics, and much more. 

Using these features for songs released between 1985 to 2015, we predict whether a song will make the Billboard Top 100 in the USA. We run both a logistic regression and random forests model. Unfortunately, while both models have a high total accuracy, they are unable to predict well whether a song will be a hit. Random forests get 5% as the best sensitivity rate.

We also evaluate how features of a Top 10 Billboard hit change over this time period, including lyric analysis. We observe that PCA does manage to group songs by lyric and acoustic features representing the popular music genres of each decade. Our biggest contribution is compiling a dataset with songs from 1958 to 2015, containing Spotify audio features and lyric features for Billboard hit songs.

##  II. Introduction

Hit song science is the possibility of predicting whether a song will be a hit before it is distributed (see [here](https://store.hbr.org/product/polyphonic-hmi-mixing-music-and-math/506009?sku=506009-PDF-ENG&N=4294935875%25204294934972). Knowing what kind of features go into a hit song help musicians, record labels, and music vendors generate larger revenues. It also increases the reach of an artist. For instance, if a record label knows that a song with particular features such as high tempo and repeated words, will have a better shot being popular, they would put more resources behind that. Similarly, a song with higher acoustic features will only appeal to certain audiences and not have mainstream appeal. 

We compiled a dataset of songs from 1958 to 2015, both Billboard Top 100 and non-hits, with their audio features from Spotify. We had a total of 431,379 observations, with 3515 hit songs. We used the dataset built by Yamac Eren Ay on [Kaggle](https://www.kaggle.com/datasets/yamaerenay/spotify-dataset-19212020-600k-tracks) as the master dataset. We added a dummy variable `billboard_hit` to indicate whether a song had been on the Billboard Top 100. For Billboard Top 100 songs, we also added lyric features from a publicly available data set by Github user `KevinSchaich`. You can find a detailed description of the dataset in Appendix 1. 

We asked two questions: 

  1. Can we predict which song will become a Billboard Top 100 hit? 
  
  2. How have Top 10 Billboard hits changed over time? 
  
For the first question, we followed the methodology laid down by Middlebrook and Sheik ([2019](https://www.arxiv-vanity.com/papers/1908.08609/)). They build four prediction models for songs from 1985 to 2015: logistic regression, neural network, random forests, and support vector machine. We use random forests and logistic regression for our analysis. 

*Random forests* combines multiple decision trees to improve predictive performance. Each decision tree is trained on a random subset of features and samples, and the final prediction is based on the majority vote of the individual trees. We will explore the use of random forests for hit song prediction and compare its performance with logistic regression.

*Logistic regression* models the probability of a binary outcome based on a set of input variables. In our case, the binary outcome is whether a song is a Billboard Top 100 hit, and the input variables are the various audio features extracted from the song.

For the second question, we evaluated how audio and lyric features have changed of Top 10 Billboard hits using Principal Component Analysis. 

## III. Predicting a Billboard Top 100 hit song 

*Random Forest*
We tested random forests for 10, 20, and 300 trees. We split our dataset into a training and test dataset in an 80-20 ratio. Given the size of the dataset, we used the foreach function on R to train multiple trees on different subsets of the data with a parallel backend from the "doParallel" package. At the cut-off level of 0.5, our random forest models all had a total accuracy rate of ~97%. But with the large number of non-song hits that we have, this is not realistice or a useful measure of accuracy. We want to look at the sensitivity or recall of the model, that is, how many songs have been correctly predicted as hits. 

For our model with 10 trees, our in-sample sensitivity is 0.4254017 or 42% and out-of-sample sensitivity is 0.03981265 or 4%. Our out-of-sample RMSE was 0.0942759. What was more concerning was percentage of variance explained: -34.23. This indicated that our model was performing very poorly, and that we were better off our null model. Increasing the trees to 20 did not help that much. We got in-sample sensitivity of 0.4433818 or 44.3% and out-of-sample sensitivity is 0.04566745 or 4.5%. Out-of-sample RMSE was 0.09235341.

```{r, echo=FALSE, out.width="70%", fig.cap="Random Forest with 300 trees", fig.align = 'left'}
knitr::include_graphics("../analysis/error_rf.png")
```

When we ran it for 300 trees, not much changed. We can see that the error dropped with increase in trees and percentage of variance explained increased to -6.46. But out-of-sample RMSE was again similar to the other models: 0.09063623. There was minute change in sensitivity, with in-sample increasing to 51.75% and out-of-sample increasing to 5.1%. 

When we generated a variable importance plot, we can see that valence is the most important in classifying the data. Energy, speechiness, and danceability also rank high.  

```{r, echo=FALSE, out.width="50%", fig.cap="Variable Importance Plot", fig.align = 'left'}
knitr::include_graphics("../analysis/var_rf.png")
```

But given these overall results, our random forests model performed quite poorly. Due to computational constraints and little marginal increase in performance, we chose to not increase the number of trees in the model. We also modeled a single decision tree, where we got an out-of-sample RMSE of 0.09712917. 

```{r, include=FALSE, rf1, echo=FALSE}
#Splitting into training and testing data
hits <- read.csv("../analysis/hits.csv")
  
bb100_split <- initial_split(hits, prop. = 0.8)
bb100_train <- training(bb100_split)
bb100_test <- testing(bb100_split)
```

```{r include=FALSE, rf2, echo=FALSE}
# let's fit a single tree
bb100.tree = rpart(hit ~ danceability + energy + key + loudness + mode + 
                     speechiness + acousticness + instrumentalness + liveness +
                     valence + tempo + duration_ms,
                   data=bb100_train, na.action = na.omit, control = rpart.control(cp = 0.00001))

cat("In-sample RMSE for single tree: ", modelr::rmse(bb100.tree, bb100_train))
cat("Out-of-sample RMSE for single tree: ", modelr::rmse(bb100.tree, bb100_test))
```

```{r rf3, include=FALSE}

##Random forests for 10 trees
cl <- makeCluster(4)
registerDoParallel(cl)
ptm <- proc.time()
bb100.forest10 <- foreach(ntree=rep(10, 1), .combine=combine, .packages='randomForest') %dopar%
  randomForest(hit ~ danceability + energy + key + loudness + mode + 
                 speechiness + acousticness + instrumentalness + liveness +
                 valence + tempo + duration_ms,  data=bb100_train, 
               na.action = na.omit, importance = TRUE, ntree=ntree, keep.inbag = TRUE)
bb100.forest10
proc.time() - ptm
stopCluster(cl)

cat("In-sample RMSE for 10 trees: ", modelr::rmse(bb100.forest10, bb100_train))
cat("Out-of-sample RMSE for 10 trees: ", modelr::rmse(bb100.forest10, bb100_test))

#Confusion matrix for 10 trees
phat_train_bb10 = predict(bb100.forest10, bb100_train) 
yhat_train_bb10 = ifelse(phat_train_bb10 > 0.5, 1, 0) 
confusion_in10 = table(y = bb100_train$hit, yhat = yhat_train_bb10) 
confusion_in10 #0.4254017 in train set
cat("In-sample sensitivity for 10 trees: ", 1112/(1112+1502))

phat_test_bb10 = predict(bb100.forest10, bb100_test) 
yhat_test_bb10 = ifelse(phat_test_bb10 > 0.5, 1, 0) 
confusion_out10 = table(y = bb100_test$hit, yhat = yhat_test_bb10) 
confusion_out10 #0.03981265 in test set
cat("Out-of-sample sensitivity for 10 trees: ", 34/(34+820))

```

```{r include=FALSE, rf4, echo=FALSE}
##Random forests for 20 trees
cl <- makeCluster(4)
registerDoParallel(cl)
ptm <- proc.time()
bb100.forest20 <- foreach(ntree=rep(10, 2), .combine=combine, .packages='randomForest') %dopar%
  randomForest(hit ~ danceability + energy + key + loudness + mode + 
                 speechiness + acousticness + instrumentalness + liveness +
                 valence + tempo + duration_ms,  data=bb100_train, 
               na.action = na.omit, importance = TRUE, ntree=ntree, keep.inbag = TRUE)
bb100.forest20
proc.time() - ptm
stopCluster(cl)

cat("In-sample RMSE for 20 trees: ", modelr::rmse(bb100.forest20, bb100_train))
cat("Out-of-sample RMSE for 20 trees: ", modelr::rmse(bb100.forest20, bb100_test))

ensemble20 <- combine(bb100.forest20)

#Confusion matrix for 20 trees
phat_train_bb20 = predict(ensemble20, bb100_train) 
yhat_train_bb20 = ifelse(phat_train_bb20 > 0.5, 1, 0) 
confusion_in20 = table(y = bb100_train$hit, yhat = yhat_train_bb20) 
confusion_in20 #0.4433818 in train set
cat("In-sample sensitivity for 20 trees: ", 1159/(1159+1455))

phat_test_bb20 = predict(ensemble20, bb100_test) 
yhat_test_bb20 = ifelse(phat_test_bb20 > 0.5, 1, 0) 
confusion_out20 = table(y = bb100_test$hit, yhat = yhat_test_bb20) 
confusion_out20 #0.04566745 in test set
cat("Out-of-sample sensitivity for 20 trees: ", 39/(39+815))
```

```{r include=FALSE, rf5, echo=FALSE}
# now a random forest for 300 trees
bb100.forest = randomForest(hit ~ danceability + energy + key + loudness + mode + 
                              speechiness + acousticness + instrumentalness + liveness +
                              valence + tempo + duration_ms,
                            data=bb100_train, ntree=300, importance = TRUE, 
                            na.action = na.omit)
plot(bb100.forest)
# let's compare RMSE on the train set
cat("In-sample RMSE for 300 trees: ", sqrt(bb100.forest$mse[length(bb100.forest$mse)]))

# let's compare RMSE on the test set
cat("Out-of-sample RMSE for 300 trees: ", modelr::rmse(bb100.forest, bb100_test))

#Confusion matrix for 300 trees
phat_train_bb1 = predict(bb100.forest, bb100_train) 
yhat_train_bb1 = ifelse(phat_train_bb1 > 0.5, 1, 0) 
confusion_in300 = table(y = bb100_train$hit, yhat = yhat_train_bb1) 
confusion_in300 #51.75% in train set
cat("In-sample sensitivity for 300 trees: ", 1353/(1353+1261))

phat_test_bb1 = predict(bb100.forest, bb100_test) 
yhat_test_bb1 = ifelse(phat_test_bb1 > 0.5, 1, 0) 
confusion_out300 = table(y = bb100_test$hit, yhat = yhat_test_bb1) 
confusion_out300 #5.1% in test set
cat("Out-of-sample sensitivity for 300 trees: ", 44/(44+810))

vi = varImpPlot(bb100.forest, type=1) #valence is the most important 
```

*Logistic Regression*

First, we build a logistic regression model to predict which song will become a Billboard Top 100 hit. `hit` is the binary variable in which 1 indicates that a song made it onto the Billboard 100 and 0 indicates that the song was not on the Billboard 100.

We will compare the out-of-sample performance of the following models:

  1. Baseline Model: a small model that uses only the `duration_ms`, `danceability`, `energy`, `key`, `loudness`, `speechiness`, `acousticness`, `instrumentalness`, `liveness`, `valence`, `tempo`, and `time_signature` variables as features with no interaction terms.
  2. Refined Model: a logistic regression model built using the forward selection process. The forward selection process considers all of the candidate variables listed above and interaction terms.


```{r classdiag, include=FALSE}
## CLASSIFICATION DIAGNOSIS
class_diag <- function(score, truth, model, test_set, positive, cutoff=.5){
  pred <- factor(score>cutoff,levels=c("TRUE","FALSE"))
  truth <- factor(truth==positive, levels=c("TRUE","FALSE"))
  tab<-table(truth, pred)
  acc=sum(diag(tab))/sum(tab)
  ppv=tab[1,1]/colSums(tab)[1]
  rmse_ = rmse(model, test_set)
  
  #CALCULATE EXACT AUC
  truth<-as.numeric(truth=="TRUE")
  ord<-order(score, decreasing=TRUE)
  score <- score[ord]; truth <- truth[ord]
  TPR=cumsum(truth)/max(1,sum(truth))
  FPR=cumsum(!truth)/max(1,sum(!truth))
  dup<-c(score[-1]>=score[-length(score)], FALSE)
  TPR<-c(0,TPR[!dup],1); FPR<-c(0,FPR[!dup],1)
  n <- length(TPR)
  auc<- sum( ((TPR[-1]+TPR[-n])/2) * (FPR[-1]-FPR[-n]) )
  round(data.frame(acc, rmse_, auc, row.names = "Metrics"),5)
}
```


#### Model Building

##### Baseline Model:

After running the baseline logistic regression model, we test for out-of-sample performance measures and see that it gives us an accuracy of 99.1%, an RMSE of about 5.295, and an AUC of 0.696. 

```{r log1, echo=FALSE}
data_hits <- read.csv("../analysis/hits.csv") %>% select(-X) %>% drop_na(duration_ms)
set.seed(500)
hits_split = initial_split(data_hits, prop=0.8)
hits_train = training(hits_split)
hits_test = testing(hits_split)
model1 = glm(hit ~ duration_ms + danceability + energy + key + loudness + speechiness + 
               acousticness + instrumentalness + liveness + valence + tempo + time_signature, 
             data = hits_train, family = "binomial")
prob_model1 = predict(model1, newdata = hits_test, type = "response")
# out-of-sample accuracy
class_diag(prob_model1, hits_test$hit, model1, hits_test, positive = 1)
```

##### Refined Model:

We build another logistic regression model using the forward selection process to get a model that includes the candidate variables and various combinations of interaction terms. Although accuracy remains the same and the RMSE score increases, we see that there is improvement in the AUC, which shows that the refined model does a better job predicting whether a song will be a Billboard hit or not.

```{r log2, echo=FALSE}
model2 = glm(hit ~ acousticness + instrumentalness + danceability + speechiness + 
               liveness + time_signature + valence + loudness + energy + 
               tempo + duration_ms + key + acousticness:speechiness + acousticness:danceability + 
               liveness:valence + valence:loudness + speechiness:loudness + 
               acousticness:energy + acousticness:loudness + valence:energy + 
               loudness:energy + acousticness:time_signature + instrumentalness:valence + 
               acousticness:instrumentalness + speechiness:liveness + danceability:time_signature + 
               danceability:valence + danceability:tempo + speechiness:tempo + 
               danceability:energy + danceability:loudness + speechiness:energy + 
               loudness:tempo + acousticness:liveness + danceability:speechiness + 
               acousticness:duration_ms + danceability:duration_ms + loudness:duration_ms + 
               instrumentalness:duration_ms + acousticness:valence + speechiness:duration_ms + 
               liveness:key + energy:key + instrumentalness:speechiness + 
               liveness:energy, 
             data=hits_train, family = "binomial")
prob_model2 = predict(model2, newdata = hits_test, type= "response")
# out-of-sample accuracy
class_diag(prob_model2, hits_test$hit, model2, hits_test, positive = 1)
```

Although accuracy is about as good as the baseline model and the AUC is improved, it is important to note that the baseline model is more parsimonious, and the trade-off between accuracy and parsimony is not substantial enough to justify using the refined model. In our case, we will stick to using the baseline model for analysis.

#### Model validation:

The following is an ROC plot for the Baseline Model, using the test dataset:

```{r log3, logistic_roc, echo=FALSE}
roc_model1 <- roc(hits_test$hit, prob_model1)
ggroc(roc_model1) + labs(title ="Figure 3: ROC Curve")
```

An ROC plots TPR vs. FPR. TPR is another name for sensitivity, while FPR is defined as (1-specificity), which explains why the numbers on the x-axis are flipped with 1 on the left and 0 on the right.

#### 10-Fold Cross Validation

In this step, we perform 10-fold cross validation for our baseline logistic regression model. For each fold, we will calculate the performance metrics, which includes accuracy, RMSE, and AUC values.

**The results for each fold is as follows:**

```{r logistic_cv, echo=FALSE}
k = 10
folds <- rep(1:k, length.out = nrow(data_hits))
diags <- NULL
i = 1
for (i in 1:k) {
  train <- data_hits[folds != i, ]
  test <- data_hits[folds == i, ]
  truth <- test$hit
  
  fit <- glm(hit == 1 ~ duration_ms + danceability + energy + key + loudness + speechiness + 
               acousticness + instrumentalness + liveness + valence + tempo + time_signature, 
                     data = train, family = "binomial")
  probs <- predict(fit, newdata = test, type = "response")
  #probs <- ifelse(probs > 0.5, 1, 0)
  diags <- rbind(diags, class_diag(probs, truth, fit, test, positive = 1))
}
diags
```

**Average of performance metrics across 10 folds:**

```{r log5, echo=FALSE}
# average performance metrics across all folds
summarize_all(diags, mean)
remove(data_hits, hits_split, hits_test, hits_train, folds, test, train)
```

From the performance metrics, we see that accuracy stays consistent at 99% on average, RMSE is about 5.29, and the AUC is at about 0.7. Although the accuracy score is high and RMSE is consistently low, the AUC indicates that performance can be further improved with a better model. 

## IV. Identifying patterns in a Top 10 hit song 

Principal Component Analysis (PCA) is a dimensionality reduction technique that can be used to identify patterns and trends in high-dimensional datasets. In the context of hit song prediction, we can use PCA to analyze how the audio and lyric features of top billboard hits have changed across decades.

First, we gathered a large dataset of songs released over several decades, including both top billboard hits. We then extract various audio features such as tempo, loudness, and danceability, as well as lyric features such as sentiment and complexity, from the songs using the Spotify Web API and natural language processing techniques. The complete data set contains the billboard top 100 hits from 1958-2015. Using the complete data set, we plot the correlation matrix for the acoustic and lyrical analysis variables, as presented below. From the matrix, as expected we see that acousticness and energy are negatively correlated while energy and loudness are positively correlated. Num_words, num_syllables, and num_lines are highly correlated, so only num_words was kept. Additionally, the various measures of sentiment (positive, negative, neutral, compound) are also highly correlated with each other, so compound_sentiment and neutral_sentiment were dropped. 

```{r all analysis, echo=FALSE}
all_data <- read.csv("https://raw.githubusercontent.com/tarinisudhakar/winningmusic/main/analysis/lyrics.csv") # 1958-2015

#Changing data integer to numeric types
all_data$danceability <- as.numeric(all_data$danceability)
all_data$energy <- as.numeric(all_data$energy)
all_data$key <- as.numeric(all_data$key)
all_data$loudness <- as.numeric(all_data$loudness)
all_data$difficult_words <- as.numeric(all_data$difficult_words)
all_data$num_dupes <- as.numeric(all_data$num_dupes)
all_data$num_words <- as.numeric(all_data$num_words)
all_data$num_lines <- as.numeric(all_data$num_lines)
all_data$duration_ms <- as.numeric(all_data$duration_ms)
all_data$mode <- as.numeric(all_data$mode)
all_data$time_signature <- as.numeric(all_data$time_signature)

#keep analysis columns
cs <- all_data[,-(33)]
cs <- cs[,-(18:19)]
cs <- cs[,-(4:5)]
cs <- cs[,-(1:2)]
cs <- na.omit(cs) # must drop na observations
cs <- cs[cs$position<=10,] #Top 10 only
X <- cs[,-(1)] # Drop position

# Center and scale the data
X = scale(X, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu = attr(X,"scaled:center")
sigma = attr(X,"scaled:scale")

#Correlation matrix
corr_matrix <- cor(X)
```


```{r correlation matrix, echo=FALSE, fig.align='left'}
ggcorrplot(corr_matrix) + 
  labs(title = "Figure 4: Correlation matrix for all Billboard Top 10 hits")

```

For each decade, we first center and scale our data. Centering the data makes sure that the clustering algorithm focuses on the patterns and differences in the data, rather than the absolute values of each variable. Scaling the data makes sure that the PCA and clustering algorithms treats each variable equally, regardless of its magnitude or unit of measurement. 

Next, we use PCA to reduce the dimensionality of the dataset by projecting it onto a lower-dimensional subspace while preserving as much of the original variability as possible. This allows us to visualize the relationships between the different features and identify the most important factors that contribute to a song's success. We generate 10 principal components of the data for each decade. We observe that P10 explains the majority of the data, with P10 values of 84, 84, 83, 84, and 81 for the 1960s, 1970s, 1980s, 1990s, and 2000s, respectively. Each decade is known to be defined for different genre of music. For each decade, we examine at the first three principal components of the top 10 ranking songs for the decade to examine the acoustic and lyrical features of the songs during that era. The summary of the PCA results for each decade can be found in the Appendix 2.

The 1960s (see Table 1) were known for Rock ‘N Roll, Motown, and The British Invasion. Various types of Rock ‘N Roll were popular during this period such as Psychedelic Rock, Blues-Rock, and Progressive Rock. Popular singers/groups included The Beatles, Bob Dylan, The Monkees, and The Rolling Stones. We see that all three PCs are actually quite similar. They all include loudness, energy, liveness, a large number of word duplications, and negative sentiment. These may be capturing different types of Rock music popular in this period. The lack of variation in the PC1-3 results may be attributed to that fact that there were not a variety of different genres popular during this period.

```{r 1960s analysis, echo=FALSE}
#Music by decade
billboard_60s = subset(all_data, year %in% c("1960", "1961", "1962", "1963", "1964", "1965", "1966", "1967", "1968", "1969"))

#keep analysis columns
cs_60s <- billboard_60s[,-(33)]
cs_60s <- cs_60s[,-(18:19)]
cs_60s <- cs_60s[,-(4:5)]
cs_60s <- cs_60s[,-(1:2)]
cs_60s <- na.omit(cs_60s) # must drop na observations
cs_60s <- cs_60s[cs_60s$position<=10,] #Top 10 only
X_60s <- cs_60s[,-(1)] # Drop position

# Center and scale the data
X_60s = scale(X_60s, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu_60s = attr(X_60s,"scaled:center")
sigma_60s = attr(X_60s,"scaled:scale")

#Num_words num_syllables num_lines are highly correlated. 
X_60s = X_60s[,-(12)]
X_60s = X_60s[,-(8)]

#Sentiment is also highly correlated with itself. Dropping compound_sentiment and neutral_sentiment. 
X_60s = X_60s[,-(3:4)]
corr_matrix_60s <- cor(X_60s)

#Trying PCA
pc_sm_60s = prcomp(X_60s, rank=10, center = TRUE, scale=TRUE)

loadings_summary_60s = pc_sm_60s$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

#PC1
pca1_60s=loadings_summary_60s %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

#PC2
pca2_60s=loadings_summary_60s %>%
  select(Category, PC2) %>%
  arrange(desc(PC2))

#PC3
pca3_60s=loadings_summary_60s %>%
  select(Category, PC3) %>%
  arrange(desc(PC3))

# Run k-means with 3 clusters and 25 starts
clust_60s = kmeans(X_60s, 3, nstart=25)
scores_60s = pc_sm_60s$x
Y_60s = as.data.frame(cbind(X_60s, scores_60s))
cluster_60s = ggplot(Y_60s) + geom_point(aes(x=PC1, y=PC5, fill=factor(clust_60s$cluster)),
                       size=3, col="#7f7f7f", shape=21) + theme_bw()
# Examine specific songs
cluster1_songs_60s=which(clust_60s$cluster == 1) 
cluster2_songs_60s=which(clust_60s$cluster == 2) 
cluster3_songs_60s=which(clust_60s$cluster == 3)
```

```{r 1960s results, echo=FALSE}
cat("Table 1: 1960s")
loadings_summary_60s[c("Category","PC1","PC2","PC3")]
```

The 1970s (see Table 2) were known for Disco, Motown, Progressive Rock, Punk, and R&B. Disco was notably one of the biggets music trends of the decade. Popular singers/groups included Diana Ross, The Jackson 5, Elton John, and Marvin Gaye. PC1, PC2, and PC3 differ considerably from each other. PC1 is largely described by acousticness, instrumentalness, and positive sentiment, perhaps picking up the rise in R&B in this decade. PC2 is described with large number of duplicate words, negative sentiment, and acousticness, perhaps a grouping of punk music that was not large, but still present in the 1970s. And finally, PC3 is described by positive sentiment, valence, danceability, energy, and loudness. This is perhaps a clear nod to the great rise in disco during the 1970s.

```{r 1970s analysis, echo=FALSE}
#Music by decade
billboard_70s = subset(all_data, year %in% c("1970", "1971", "1972", "1973", "1974", "1975", "1976", "1977", "1978", "1979"))

#keep analysis columns
cs_70s <- billboard_70s[,-(33)]
cs_70s <- cs_70s[,-(18:19)]
cs_70s <- cs_70s[,-(4:5)]
cs_70s <- cs_70s[,-(1:2)]
cs_70s <- na.omit(cs_70s) # must drop na observations
cs_70s <- cs_70s[cs_70s$position<=10,] #Top 10 only
X_70s <- cs_70s[,-(1)] # Drop position

# Center and scale the data
X_70s = scale(X_70s, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu_70s = attr(X_70s,"scaled:center")
sigma_70s = attr(X_70s,"scaled:scale")

#Num_words num_syllables num_lines are highly correlated. 
X_70s = X_70s[,-(12)]
X_70s = X_70s[,-(8)]

#Sentiment is also highly correlated with itself. Dropping compound_sentiment and neutral_sentiment. 
X_70s = X_70s[,-(3:4)]
corr_matrix_70s <- cor(X_70s)

#Trying PCA
pc_sm_70s = prcomp(X_70s, rank=10, center = TRUE, scale=TRUE)

loadings_summary_70s = pc_sm_70s$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

#PC1
pca1_70s=loadings_summary_70s %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

#PC2
pca2_70s=loadings_summary_70s %>%
  select(Category, PC2) %>%
  arrange(desc(PC2))

#PC3
pca3_70s=loadings_summary_70s %>%
  select(Category, PC3) %>%
  arrange(desc(PC3))

# Run k-means with 3 clusters and 25 starts
clust_70s = kmeans(X_70s, 3, nstart=25)
scores_70s = pc_sm_70s$x
Y_70s = as.data.frame(cbind(X_70s, scores_70s))
cluster_70s = ggplot(Y_70s) + geom_point(aes(x=PC1, y=PC5, fill=factor(clust_70s$cluster)),
                       size=3, col="#7f7f7f", shape=21) + theme_bw()
# Examine specific songs
cluster1_songs_70s=which(clust_70s$cluster == 1) 
cluster2_songs_70s=which(clust_70s$cluster == 2) 
cluster3_songs_70s=which(clust_70s$cluster == 3)
```

```{r 1970s results, echo=FALSE}
cat("Table 2: 1970s")
loadings_summary_70s[c("Category","PC1","PC2","PC3")]
```

The 1980s (see Table 3) were known for Pop, Heavy Metal, Glam rock, Hip-Hop, and Country. Popular singers/groups included Whitney Houston, Madonna, Dolly Parton, Michael Jackson, Lionel Ritchie, MC Hammer, and Bon Jovi. In the 1980's music was dramatically changed by the introduction of MTV (Music Television) where a greater importance was placed on the appearance of musicians and gimmicks became commonplace. Hip-Hop also came into the mainstream during this decade. Again, PC1, PC2, and PC3 differ considerably from each other. PC1 is described by a larger number of words, energy, loudness, liveness, and speechiness, perhaps picking up the rise in hip hop during this decade. PC2 is described by neutral sentiment a larger number of words, liveness, acousticness, and instrumentalness. This grouping is more difficult to pin point, though it may be a point to country music or a grouping of pop music during this period. And finally, PC3 is described by negative sentiment, loudness, energy, higher tempo, and liveness. This may be describing a heavy metal and/or a rock grouping during this decade. 

```{r 1980s analysis, echo=FALSE}
#Music by decade
billboard_80s = subset(all_data, year %in% c("1980", "1981", "1982", "1983", "1984", "1985", "1986", "1987", "1988", "1989"))

#keep analysis columns
cs_80s <- billboard_80s[,-(33)]
cs_80s <- cs_80s[,-(18:19)]
cs_80s <- cs_80s[,-(4:5)]
cs_80s <- cs_80s[,-(1:2)]
cs_80s <- na.omit(cs_80s) # must drop na observations
cs_80s <- cs_80s[cs_80s$position<=10,] #Top 10 only
X_80s <- cs_80s[,-(1)] # Drop position

# Center and scale the data
X_80s = scale(X_80s, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu_80s = attr(X_80s,"scaled:center")
sigma_80s = attr(X_80s,"scaled:scale")

#Num_words num_syllables num_lines are highly correlated. 
X_80s = X_80s[,-(12)]
X_80s = X_80s[,-(8)]

#Sentiment is also highly correlated with itself. Dropping compound_sentiment and neutral_sentiment. 
X_80s = X_80s[,-(3:4)]
corr_matrix_80s <- cor(X_80s)
X_80s = X_80s[,-(21)] #No variance in time_signature
#Trying PCA
pc_sm_80s = prcomp(X_80s, rank=10, center = TRUE, scale=TRUE)

loadings_summary_80s = pc_sm_80s$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

#PC1
pca1_80s=loadings_summary_80s %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

#PC2
pca2_80s=loadings_summary_80s %>%
  select(Category, PC2) %>%
  arrange(desc(PC2))

#PC3
pca3_80s=loadings_summary_80s %>%
  select(Category, PC3) %>%
  arrange(desc(PC3))

# Run k-means with 3 clusters and 25 starts
clust_80s = kmeans(X_80s, 3, nstart=25)
scores_80s = pc_sm_80s$x
Y_80s = as.data.frame(cbind(X_80s, scores_80s))
cluster_80s = ggplot(Y_80s) + geom_point(aes(x=PC1, y=PC5, fill=factor(clust_80s$cluster)),
                       size=3, col="#7f7f7f", shape=21) + theme_bw()
# Examine specific songs
cluster1_songs_80s=which(clust_80s$cluster == 1) 
cluster2_songs_80s=which(clust_80s$cluster == 2) 
cluster3_songs_80s=which(clust_80s$cluster == 3)
```
```{r 1980s results, echo=FALSE}
cat("Table 3: 1980s")
loadings_summary_80s[c("Category","PC1","PC2","PC3")]
```

The 1990s (see Table 4) were known for Grunge, Rap, Hip-Hop, Bubblegum Pop, Contemporary R&B, and Country. Popular singers/groups included Nirvana, MC Hammer, Britney Spears, Backstreet Boys, N'Sync, Red Hot Chili Peppers, 2Pac, Notorious B.I.G., and Guns & Roses. Very clearly, the 1990's was an era filled with all different genre's of music. PC1 is described by acousticness, positive sentiment, longer length, and higher flesch_index scores (that is, the song lyrics are not of high difficulty to understand). This grouping may be an description of Contemporary R&B or Country. PC2 is described by loudness, energy, valence, and positive sentiment, perhaps pointing to pop songs during this period. And PC3 is described by positiveness, liveness, acousticness, and speechiness, which would describe a grouping of the Contemporary R&B present in the 90s.

```{r 1990s analysis, echo=FALSE}
#Music by decade
billboard_90s = subset(all_data, year %in% c("1990", "1991", "1992", "1993", "1994", "1995", "1996", "1997", "1998", "1999"))

#keep analysis columns
cs_90s <- billboard_90s[,-(33)]
cs_90s <- cs_90s[,-(18:19)]
cs_90s <- cs_90s[,-(4:5)]
cs_90s <- cs_90s[,-(1:2)]
cs_90s <- na.omit(cs_90s) # must drop na observations
cs_90s <- cs_90s[cs_90s$position<=10,] #Top 10 only
X_90s <- cs_90s[,-(1)] # Drop position

# Center and scale the data
X_90s = scale(X_90s, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu_90s = attr(X_90s,"scaled:center")
sigma_90s = attr(X_90s,"scaled:scale")

#Num_words num_syllables num_lines are highly correlated. 
X_90s = X_90s[,-(12)]
X_90s = X_90s[,-(8)]

#Sentiment is also highly correlated with itself. Dropping compound_sentiment and neutral_sentiment. 
X_90s = X_90s[,-(3:4)]
corr_matrix_90s <- cor(X_90s)

#Trying PCA
pc_sm_90s = prcomp(X_90s, rank=10, center = TRUE, scale=TRUE)

loadings_summary_90s = pc_sm_90s$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

#PC1
pca1_90s=loadings_summary_90s %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

#PC2
pca2_90s=loadings_summary_90s %>%
  select(Category, PC2) %>%
  arrange(desc(PC2))

#PC3
pca3_90s=loadings_summary_90s %>%
  select(Category, PC3) %>%
  arrange(desc(PC3))

# Run k-means with 3 clusters and 25 starts
clust_90s = kmeans(X_90s, 3, nstart=25)
scores_90s = pc_sm_90s$x
Y_90s = as.data.frame(cbind(X_90s, scores_90s))
cluster_90s = ggplot(Y_90s) + geom_point(aes(x=PC1, y=PC5, fill=factor(clust_90s$cluster)),
                       size=3, col="#7f7f7f", shape=21) + theme_bw()
# Examine specific songs
cluster1_songs_90s=which(clust_90s$cluster == 1) 
cluster2_songs_90s=which(clust_90s$cluster == 2) 
cluster3_songs_90s=which(clust_90s$cluster == 3)
```

```{r 1990s results, echo=FALSE}
cat("Table 4: 1990s")
loadings_summary_90s[c("Category","PC1","PC2","PC3")]
```

The 2000s (see Table 5) were known for Pop, Hip-Hop, R&B, and Rock. Popular singers/groups included Eminem, Kanye West, Nickelback, Beyoncé, Britney Spears, The Killers, and Linkin Park. The PCs all share energy and loudness features. However, PC1 is additionally described by a larger number of words, higher tempo, and more word duplications (repitition), perhaps an indication of the large presence of hip hop during this period. PC2 is additionally described by instrumentalness, liveness, valence, and negative sentiment, perhaps an indication of a section of Rock. And finally, PC3 is additionally described by liveness, higher tempo, more difficult words, and negative sentiment. This grouping is a but more difficult to pin down, though the grouping may again be referring to Rock, but perhaps a different variety (Rock was still very popular during this period with many different genres such as post-grunge, pop punk, post-hardcore, metalcore, and indie). But very clearly, the 2000s were defined by energy and loudness in their top hits.

```{r 2000s analysis, echo=FALSE}
#Music by decade
billboard_00s = subset(all_data, year %in% c("2000", "2001", "2002", "2003", "2004", "2005", "2006", "2007", "2008", "2009"))

#keep analysis columns
cs_00s <- billboard_00s[,-(33)]
cs_00s <- cs_00s[,-(18:19)]
cs_00s <- cs_00s[,-(4:5)]
cs_00s <- cs_00s[,-(1:2)]
cs_00s <- na.omit(cs_00s) # must drop na observations
cs_00s <- cs_00s[cs_00s$position<=10,] #Top 10 only
X_00s <- cs_00s[,-(1)] # Drop position

# Center and scale the data
X_00s = scale(X_00s, center=TRUE, scale=TRUE)

# Extract the centers and scales from the rescaled data (which are named attributes)
mu_00s = attr(X_00s,"scaled:center")
sigma_00s = attr(X_00s,"scaled:scale")

#Num_words num_syllables num_lines are highly correlated. 
X_00s = X_00s[,-(12)]
X_00s = X_00s[,-(8)]

#Sentiment is also highly correlated with itself. Dropping compound_sentiment and neutral_sentiment. 
X_00s = X_00s[,-(3:4)]
corr_matrix_00s <- cor(X_00s)

#Trying PCA
pc_sm_00s = prcomp(X_00s, rank=10, center = TRUE, scale=TRUE)

loadings_summary_00s = pc_sm_00s$rotation %>%
  as.data.frame() %>%
  rownames_to_column('Category')

#PC1
pca1_00s=loadings_summary_00s %>%
  select(Category, PC1) %>%
  arrange(desc(PC1))

#PC2
pca2_00s=loadings_summary_00s %>%
  select(Category, PC2) %>%
  arrange(desc(PC2))

#PC3
pca3_00s=loadings_summary_00s %>%
  select(Category, PC3) %>%
  arrange(desc(PC3))

# Run k-means with 3 clusters and 25 starts
clust_00s = kmeans(X_00s, 3, nstart=25)
scores_00s = pc_sm_00s$x
Y_00s = as.data.frame(cbind(X_00s, scores_00s))
cluster_00s = ggplot(Y_00s) + geom_point(aes(x=PC1, y=PC5, fill=factor(clust_00s$cluster)),
                       size=3, col="#7f7f7f", shape=21) + theme_bw()
# Examine specific songs
cluster1_songs_00s=which(clust_00s$cluster == 1) 
cluster2_songs_00s=which(clust_00s$cluster == 2) 
cluster3_songs_00s=which(clust_00s$cluster == 3)
```

```{r 2000s results, echo=FALSE}
cat("Table 5: 2000s")
loadings_summary_00s[c("Category","PC1","PC2","PC3")]
```

##  V. Conclusion

*Hit song prediction* 
For the logistic regression, the baseline model's performance is comparable to the refined model's performance. The refined model has a slightly higher AUC, but there is no significant improvement in total accuracy or RMSE. Because the improvement in the model is not significant enough to justify losing parsimony from the baseline model, we conclude that the baseline model is the best choice for logistic regression modeling. 

In the random forests model, increasing the number of trees led to marginal improvement in the model with large jumps in computational burden. The RMSE for random forests with 300 trees barely differed from that of a single decision tree. But it did get a sensitivity rate of 5%, whereas the logistic regression failed to predict any hit songs. We understand this is due to a major class imbalance, where we have several more observations of non-hit songs than we have of hit songs. Once we correct for that, we can tweak the random forests model by playing with maximum depth, Gini index and more. 

*Patterns in Top 10 hits* 
While tastes for different genre of music has changed across decades, we see that certain characteristics, such as energy and loudness, continue to be important features of chart topping hits. We also observe variations in principal components that arise as different genres of music emerged through the decades. Overall, using PCA to analyze how the audio and lyric features of Top Billboard hits have changed across decades can provide valuable insights into the evolution of popular music and help inform future hit song predictions.

## Appendix 1
### Data Sources
The publicly available data set by Github user `KevinSchaich` analyzed the lyrics of Billboard's Top 100 songs from 1950-2015 using a variety of Natural Language Processing techniques. The description of the variables obtained from this data set are presented below:

* year: Release year of the song
* position: Position of Billboard's Top 100 for the given year
* title: Title of the song 
* artist: Artist of the song
* pos_sentiment: Positivity association with lyrics. Value ranges between 0-1 inclusive, with 1 being 100% positive.
* neg_sentiment: Negativity association with lyrics. Value ranges between 0-1 inclusive, with 1 being 100% negative.
* neut_sentiment: Neutrality association with lyrics. Value ranges between 0-1 inclusive, with 1 being 100% neutral.
* compound_sentiment: The sum of positive, negative, and neutral scores which is then normalized between -1 (most extreme negative) and +1 (most extreme positive).
* f_k_grade: Flesch–Kincaid grade level of the song's lyrics. The Flesch-Kincaid Grade Level is equivalent to the US grade level of education.
* flesch_index: Flesch reading ease score of the song's lyrics. The Flesch reading ease score indicates the understandability of a passage with a number that ranges from 0 to 100. Higher scores indicate that the content is easier to read and understand.
* fog_index: Gunning-Fog readability index estimates the years of formal education a person needs to understand the text on the first reading. 
* num_syllables: Number of syllables in lyrics
* difficult_words: Number of words not on the Dale–Chall "easy" word list
* num_dupes: Number of duplicate (repetitive) lines in lyrics
* num_words: Number of words in lyrics
* num_lines: Number of lines in lyrics
* genre_tags: song artist's associated genre tags

The positive, negative, neutral, and compound sentiment scores of each song's lyrics were gathered using Python's Natural Language Toolkit (NLTK) VADER model. Readability metrics for each song's lyrics were obtained using the textstat package for Python. Each song artist's associated genre tags were scraped using MusicBrainz API as well as the Musicbrainzng Python interface.

Due to data availability limitations, the data set provides approximately 80-90% coverage for all the songs on Billboard's list from 1950-2015.

We obtain audio features of each song in our billboard data set using Spotify's Web API. The audio features of each song are listed below:

* danceability: Danceability describes how suitable a track is for dancing based on a combination of musical elements including tempo, rhythm stability, beat strength, and overall regularity. A value of 0.0 is least danceable and 1.0 is most danceable.
* energy: Energy is a measure from 0.0 to 1.0 and represents a perceptual measure of intensity and activity. 
* key: The key the track is in. Integers map to pitches using standard Pitch Class notation.
* loudness: The overall loudness of a track in decibels (dB). Loudness values are averaged across the entire track and are useful for comparing relative loudness of tracks. Values typical range between -60 and 0 db.
* mode: Mode indicates the modality (major or minor) of a track, the type of scale from which its melodic content is derived. Major is represented by 1 and minor is 0.
* speechiness: Speechiness detects the presence of spoken words in a track. The more exclusively speech-like the recording, the closer to 1.0 the attribute value.
* acousticness: A confidence measure from 0.0 to 1.0 of whether the track is acoustic. 1.0 represents high confidence the track is acoustic.
* instrumentalness: Predicts whether a track contains no vocals. The closer the instrumentalness value is to 1.0, the greater likelihood the track contains no vocal content.
* liveness: Detects the presence of an audience in the recording. Higher liveness values represent an increased probability that the track was performed live. 
* valence: A measure from 0.0 to 1.0 describing the musical positiveness conveyed by a track. Tracks with high valence sound more positive (e.g. happy, cheerful, euphoric), while tracks with low valence sound more negative (e.g. sad, depressed, angry).
* tempo: The overall estimated tempo of a track in beats per minute (BPM).
* duration_ms: The duration of the track in milliseconds.
* time_signature: An estimated overall time signature of a track. The time signature (meter) is a notational convention to specify how many beats are in each bar (or measure).


## Appendix 2
### PCA 1960s 
```{r appendix 1960s, echo=FALSE}
summary(pc_sm_60s)
```

### PCA 1970s 
```{r appendix 1970s, echo=FALSE}
summary(pc_sm_70s)
```

### PCA 1980s 
```{r appendix 1980s, echo=FALSE}
summary(pc_sm_80s)
```

### PCA 1990s 
```{r appendix 1990s, echo=FALSE}
summary(pc_sm_90s)
```

### PCA 2000s 
```{r appendix 2000s, echo=FALSE}
summary(pc_sm_00s)
```
